{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pruno3D next-frame with conv_lstm","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/conv_lstm.ipynb","timestamp":1618131564887}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nCRGMSO5KCzV"},"source":["# Next-frame prediction with Conv-LSTM\n","\n","**Author:** [jeammimi](https://github.com/jeammimi)<br>\n","**Date created:** 2016/11/02<br>\n","**Last modified:** 2020/05/01<br>\n","**Description:** Predict the next frame in a sequence using a Conv-LSTM model."]},{"cell_type":"markdown","metadata":{"id":"MSwx-vnsKCzc"},"source":["## Introduction\n","\n","This script demonstrates the use of a convolutional LSTM model.\n","The model is used to predict the next frame of an artificially\n","generated movie which contains moving squares.\n"]},{"cell_type":"markdown","metadata":{"id":"Yj2hP5xTKCzd"},"source":["## Setup\n"]},{"cell_type":"code","metadata":{"id":"1JArwR_ZKCzd","executionInfo":{"status":"ok","timestamp":1618203556256,"user_tz":420,"elapsed":2833,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","import pylab as plt\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"om4j8dIIKCze"},"source":["## Build a model\n","\n","We create a model which take as input movies of shape\n","`(n_frames, width, height, channels)` and returns a movie\n","of identical shape.\n"]},{"cell_type":"code","metadata":{"id":"Id3bCcjBKCze","executionInfo":{"status":"ok","timestamp":1618203557051,"user_tz":420,"elapsed":3619,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["seq = keras.Sequential(\n","    [\n","        keras.Input(\n","            shape=(None, 40, 40, 1)\n","        ),  # Variable-length sequence of 40x40x1 frames\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        # layers.SpatialDropout3D(0.2),\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        layers.Conv3D(\n","            filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n","        ),\n","    ]\n",")\n","seq.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTm2CT3-KCzf"},"source":["## Generate artificial data\n","\n","Generate movies with 3 to 7 moving squares inside.\n","The squares are of shape 1x1 or 2x2 pixels,\n","and move linearly over time.\n","For convenience, we first create movies with bigger width and height (80x80)\n","and at the end we select a 40x40 window.\n"]},{"cell_type":"code","metadata":{"id":"i4frnPm7KCzf","executionInfo":{"status":"ok","timestamp":1618203557052,"user_tz":420,"elapsed":3612,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","def generate_movies(n_samples=1200, n_frames=15):\n","    row = 80\n","    col = 80\n","    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n","    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n","\n","    for i in range(n_samples):\n","        # Add 3 to 7 moving squares\n","        n = np.random.randint(3, 8)\n","\n","        for j in range(n):\n","            # Initial position\n","            xstart = np.random.randint(20, 60)\n","            ystart = np.random.randint(20, 60)\n","            # Direction of motion\n","            directionx = np.random.randint(0, 3) - 1\n","            directiony = np.random.randint(0, 3) - 1\n","\n","            # Size of the square\n","            w = np.random.randint(2, 4)\n","\n","            for t in range(n_frames):\n","                x_shift = xstart + directionx * t\n","                y_shift = ystart + directiony * t\n","                noisy_movies[\n","                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n","                ] += 1\n","\n","                # Make it more robust by adding noise.\n","                # The idea is that if during inference,\n","                # the value of the pixel is not exactly one,\n","                # we need to train the model to be robust and still\n","                # consider it as a pixel belonging to a square.\n","                if np.random.randint(0, 2):\n","                    noise_f = (-1) ** np.random.randint(0, 2)\n","                    noisy_movies[\n","                        i,\n","                        t,\n","                        x_shift - w - 1 : x_shift + w + 1,\n","                        y_shift - w - 1 : y_shift + w + 1,\n","                        0,\n","                    ] += (noise_f * 0.1)\n","\n","                # Shift the ground truth by 1\n","                x_shift = xstart + directionx * (t + 1)\n","                y_shift = ystart + directiony * (t + 1)\n","                shifted_movies[\n","                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n","                ] += 1\n","\n","    # Cut to a 40x40 window\n","    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n","    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n","    noisy_movies[noisy_movies >= 1] = 1\n","    shifted_movies[shifted_movies >= 1] = 1\n","    return noisy_movies, shifted_movies\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHhDhYVoKCzg"},"source":["## Train the model\n"]},{"cell_type":"code","metadata":{"id":"5aM_1KhEKCzg","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1618204310952,"user_tz":420,"elapsed":757504,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}},"outputId":"51fe2426-87b1-4cdf-9187-75209c147005"},"source":["epochs = 200\n","\n","earlystopping = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, min_delta=1e-4)\n","\n","noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n","seq.fit(\n","    noisy_movies[:1000],\n","    shifted_movies[:1000],\n","    batch_size=30,\n","    epochs=epochs,\n","    verbose=2,\n","    validation_split=0.1,\n","    callbacks=[earlystopping]\n",")\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","45/45 - 32s - loss: nan - val_loss: nan\n","Epoch 2/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 3/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 4/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 5/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 6/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 7/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 8/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 9/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 10/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 11/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 12/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 13/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 14/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 15/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 16/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 17/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 18/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 19/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 20/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 21/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 22/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 23/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 24/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 25/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 26/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 27/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 28/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 29/200\n","45/45 - 25s - loss: nan - val_loss: nan\n","Epoch 30/200\n","45/45 - 25s - loss: nan - val_loss: nan\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-defbf6522988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlystopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1776\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Restoring model weights from the end of the best epoch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0mexpected_num_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mexpected_num_weights\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m       raise ValueError(\n\u001b[1;32m   1854\u001b[0m           \u001b[0;34m'You called `set_weights(weights)` on layer \"%s\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"markdown","metadata":{"id":"vi1IeBFHKCzh"},"source":["## Test the model on one movie\n","\n","Feed it with the first 7 positions and then\n","predict the new positions.\n"]},{"cell_type":"code","metadata":{"id":"IhiPimtxKCzh","executionInfo":{"status":"aborted","timestamp":1618204310949,"user_tz":420,"elapsed":757491,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["movie_index = 1004\n","track = noisy_movies[movie_index][:7, ::, ::, ::]\n","\n","for j in range(16):\n","    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n","    new = new_pos[::, -1, ::, ::, ::]\n","    track = np.concatenate((track, new), axis=0)\n","\n","\n","# And then compare the predictions\n","# to the ground truth\n","track2 = noisy_movies[movie_index][::, ::, ::, ::]\n","for i in range(15):\n","    fig = plt.figure(figsize=(10, 5))\n","\n","    ax = fig.add_subplot(121)\n","\n","    if i >= 7:\n","        ax.text(1, 3, \"Predictions !\", fontsize=20, color=\"w\")\n","    else:\n","        ax.text(1, 3, \"Initial trajectory\", fontsize=20)\n","\n","    toplot = track[i, ::, ::, 0]\n","\n","    plt.imshow(toplot)\n","    ax = fig.add_subplot(122)\n","    plt.text(1, 3, \"Ground truth\", fontsize=20)\n","\n","    toplot = track2[i, ::, ::, 0]\n","    if i >= 2:\n","        toplot = shifted_movies[movie_index][i - 1, ::, ::, 0]\n","\n","    plt.imshow(toplot)\n","    plt.savefig(\"%i_animate.png\" % (i + 1))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qck_1WgjVOPp"},"source":["# Pruno3D"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8G3qufXpak6W","executionInfo":{"status":"ok","timestamp":1618206036625,"user_tz":420,"elapsed":5841,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}},"outputId":"7e467e11-db4f-44f1-d2c2-9ab3d565b7e0"},"source":["!pip install -q git+https://github.com/LanceNorskog/keras-pruno.git\n","from keras_pruno import Pruno3D"],"execution_count":5,"outputs":[{"output_type":"stream","text":["  Building wheel for keras-pruno (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOrEnGO9Va2C","executionInfo":{"status":"ok","timestamp":1618206036626,"user_tz":420,"elapsed":5828,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}},"outputId":"079ce698-5a52-4874-e160-aed3812f7c5f"},"source":["seq = keras.Sequential(\n","    [\n","        keras.Input(\n","            shape=(None, 40, 40, 1)\n","        ),  # Variable-length sequence of 40x40x1 frames\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        layers.ConvLSTM2D(\n","            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n","        ),\n","        layers.BatchNormalization(),\n","        Pruno3D(similarity=0.65),\n","        layers.Conv3D(\n","            filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n","        ),\n","    ]\n",")\n","seq.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["__init__: similarity: 0.65\n","Pruno3D.build: input_shape: (None, None, 40, 40, 40)\n","Pruno3D.build: self.fmap_shape: (None, 40, 40)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faWuYIqgVvUU","outputId":"2bced0a1-2367-4fe9-f1fc-b11511e55402"},"source":["epochs = 200\n","\n","earlystopping = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, min_delta=1e-4)\n","\n","noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n","seq.fit(\n","    noisy_movies[:1000],\n","    shifted_movies[:1000],\n","    batch_size=30,\n","    epochs=epochs,\n","    verbose=2,\n","    validation_split=0.1,\n","    callbacks=[earlystopping]\n",")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","Pruno3D.call: type inputs.shape: <class 'tensorflow.python.framework.tensor_shape.TensorShape'>\n","Pruno3D.call: inputs.shape: (20, 15, 40, 40, 40)\n","Pruno3D.call: self.fmap_shape: (None, 40, 40)\n","actual_batchsize: Tensor(\"sequential_1/pruno3d/strided_slice:0\", shape=(1,), dtype=int32)\n","Pruno3D.call: type inputs.shape: <class 'tensorflow.python.framework.tensor_shape.TensorShape'>\n","Pruno3D.call: inputs.shape: (20, 15, 40, 40, 40)\n","Pruno3D.call: self.fmap_shape: ListWrapper([15, 40, 40])\n","actual_batchsize: Tensor(\"sequential_1/pruno3d/strided_slice:0\", shape=(1,), dtype=int32)\n","45/45 - 33s - loss: 0.6931 - val_loss: 0.6944\n","Epoch 2/200\n","45/45 - 26s - loss: 0.6931 - val_loss: 0.7003\n","Epoch 3/200\n","45/45 - 26s - loss: 0.6931 - val_loss: 0.7135\n","Epoch 4/200\n","45/45 - 26s - loss: 0.6930 - val_loss: 0.7371\n","Epoch 5/200\n","45/45 - 26s - loss: 0.6930 - val_loss: 0.7732\n","Epoch 6/200\n","45/45 - 26s - loss: 0.6930 - val_loss: 0.8215\n","Epoch 7/200\n","45/45 - 26s - loss: 0.6929 - val_loss: 0.8766\n","Epoch 8/200\n","45/45 - 26s - loss: 0.6929 - val_loss: 0.9282\n","Epoch 9/200\n","45/45 - 26s - loss: 0.6928 - val_loss: 0.9658\n","Epoch 10/200\n","45/45 - 26s - loss: 0.6928 - val_loss: 0.9829\n","Epoch 11/200\n","45/45 - 26s - loss: 0.6928 - val_loss: 0.9798\n","Epoch 12/200\n","45/45 - 26s - loss: 0.6927 - val_loss: 0.9622\n","Epoch 13/200\n","45/45 - 26s - loss: 0.6927 - val_loss: 0.9361\n","Epoch 14/200\n","45/45 - 26s - loss: 0.6926 - val_loss: 0.9106\n","Epoch 15/200\n","45/45 - 26s - loss: 0.6926 - val_loss: 0.8911\n","Epoch 16/200\n","45/45 - 26s - loss: 0.6925 - val_loss: 0.8786\n","Epoch 17/200\n","45/45 - 26s - loss: 0.6925 - val_loss: 0.8712\n","Epoch 18/200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tDQYYq5VVwyl"},"source":[""],"execution_count":null,"outputs":[]}]}