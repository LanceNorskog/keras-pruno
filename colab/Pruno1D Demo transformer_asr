{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pruno1D Demo transformer_asr","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/audio/ipynb/transformer_asr.ipynb","timestamp":1618292877490}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0REZh2eVxS5W"},"source":["# Automatic Speech Recognition with Transformer\n","\n","**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n","**Date created:** 2021/01/13<br>\n","**Last modified:** 2021/01/13<br>\n","**Description:** Training a sequence-to-sequence Transformer for automatic speech recognition."]},{"cell_type":"markdown","metadata":{"id":"a-gi25vixS5e"},"source":["## Introduction\n","\n","Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n","ASR can be treated as a sequence-to-sequence problem, where the\n","audio can be represented as a sequence of feature vectors\n","and the text as a sequence of characters, words, or subword tokens.\n","\n","For this demonstration, we will use the LJSpeech dataset from the\n","[LibriVox](https://librivox.org/) project. It consists of short\n","audio clips of a single speaker reading passages from 7 non-fiction books.\n","Our model will be similar to the original Transformer (both encoder and decoder)\n","as proposed in the paper, \"Attention is All You Need\".\n","\n","\n","**References:**\n","\n","- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n","- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n","- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n","- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"]},{"cell_type":"code","metadata":{"id":"MKldHskZxS5f","executionInfo":{"status":"ok","timestamp":1618309742193,"user_tz":420,"elapsed":1644,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","import os\n","import random\n","from glob import glob\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZOpS1UuxS5f"},"source":["## Define the Transformer Input Layer\n","\n","When processing past target tokens for the decoder, we compute the sum of\n","position embeddings and token embeddings.\n","\n","When processing audio features, we apply convolutional layers to downsample\n","them (via convolution stides) and process local relationships."]},{"cell_type":"code","metadata":{"id":"1ApevpBjxS5g","executionInfo":{"status":"ok","timestamp":1618309742197,"user_tz":420,"elapsed":1638,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class TokenEmbedding(layers.Layer):\n","    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n","        super().__init__()\n","        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        x = self.emb(x)\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions\n","\n","\n","class SpeechFeatureEmbedding(layers.Layer):\n","    def __init__(self, num_hid=64, maxlen=100):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv2 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv3 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return self.conv3(x)\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7USR0gRjxS5g"},"source":["## Transformer Encoder Layer"]},{"cell_type":"code","metadata":{"id":"z3PTXmOhxS5g","executionInfo":{"status":"ok","timestamp":1618309742198,"user_tz":420,"elapsed":1632,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sihr4wwtxS5h"},"source":["## Transformer Decoder Layer"]},{"cell_type":"code","metadata":{"id":"I4eVxRlnxS5h","executionInfo":{"status":"ok","timestamp":1618309742198,"user_tz":420,"elapsed":1625,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n","        super().__init__()\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","        self.self_att = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.self_dropout = layers.Dropout(0.5)\n","        self.enc_dropout = layers.Dropout(0.1)\n","        self.ffn_dropout = layers.Dropout(0.1)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","\n","    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n","        \"\"\"Masks the upper half of the dot product matrix in self attention.\n","\n","        This prevents flow of information from future tokens to current token.\n","        1's in the lower triangle, counting from the lower right corner.\n","        \"\"\"\n","        i = tf.range(n_dest)[:, None]\n","        j = tf.range(n_src)\n","        m = i >= j - n_src + n_dest\n","        mask = tf.cast(m, dtype)\n","        mask = tf.reshape(mask, [1, n_dest, n_src])\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n","        )\n","        return tf.tile(mask, mult)\n","\n","    def call(self, enc_out, target):\n","        input_shape = tf.shape(target)\n","        batch_size = input_shape[0]\n","        seq_len = input_shape[1]\n","        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        target_att = self.self_att(target, target, attention_mask=causal_mask)\n","        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n","        enc_out = self.enc_att(target_norm, enc_out)\n","        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n","        ffn_out = self.ffn(enc_out_norm)\n","        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n","        return ffn_out_norm\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ulPvv8dPxS5i"},"source":["## Complete the Transformer model\n","\n","Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n","During training, we give the decoder the target character sequence shifted to the left\n","as input. During inference, the decoder uses its own past predictions to predict the\n","next token."]},{"cell_type":"code","metadata":{"id":"iYLzvbq_xS5i","executionInfo":{"status":"ok","timestamp":1618309742325,"user_tz":420,"elapsed":1746,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class Transformer(keras.Model):\n","    def __init__(\n","        self,\n","        num_hid=64,\n","        num_head=2,\n","        num_feed_forward=128,\n","        source_maxlen=100,\n","        target_maxlen=100,\n","        num_layers_enc=4,\n","        num_layers_dec=1,\n","        num_classes=10,\n","    ):\n","        super().__init__()\n","        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n","        self.num_layers_enc = num_layers_enc\n","        self.num_layers_dec = num_layers_dec\n","        self.target_maxlen = target_maxlen\n","        self.num_classes = num_classes\n","\n","        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n","        self.dec_input = TokenEmbedding(\n","            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n","        )\n","\n","        self.encoder = keras.Sequential(\n","            [self.enc_input]\n","            + [\n","                TransformerEncoder(num_hid, num_head, num_feed_forward)\n","                for _ in range(num_layers_enc)\n","            ]\n","        )\n","\n","        for i in range(num_layers_dec):\n","            setattr(\n","                self,\n","                f\"dec_layer_{i}\",\n","                TransformerDecoder(num_hid, num_head, num_feed_forward),\n","            )\n","\n","        self.classifier = layers.Dense(num_classes)\n","\n","    def decode(self, enc_out, target):\n","        y = self.dec_input(target)\n","        for i in range(self.num_layers_dec):\n","            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n","        return y\n","\n","    def call(self, inputs):\n","        source = inputs[0]\n","        target = inputs[1]\n","        x = self.encoder(source)\n","        y = self.decode(x, target)\n","        return self.classifier(y)\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_metric]\n","\n","    def train_step(self, batch):\n","        \"\"\"Processes one batch inside model.fit().\"\"\"\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        with tf.GradientTape() as tape:\n","            preds = self([source, dec_input])\n","            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def test_step(self, batch):\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        preds = self([source, dec_input])\n","        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def generate(self, source, target_start_token_idx):\n","        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n","        bs = tf.shape(source)[0]\n","        enc = self.encoder(source)\n","        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n","        dec_logits = []\n","        for i in range(self.target_maxlen - 1):\n","            dec_out = self.decode(enc, dec_input)\n","            logits = self.classifier(dec_out)\n","            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n","            dec_logits.append(last_logit)\n","            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n","        return dec_input\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJ4IP8rhxS5j"},"source":["## Download the dataset\n","\n","Note: This requires ~3.6 GB of disk space and\n","takes ~5 minutes for the extraction of files."]},{"cell_type":"code","metadata":{"id":"p5r2MRAkxS5k","executionInfo":{"status":"ok","timestamp":1618310009797,"user_tz":420,"elapsed":269212,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["keras.utils.get_file(\n","    os.path.join(os.getcwd(), \"data.tar.gz\"),\n","    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n","    extract=True,\n","    archive_format=\"tar\",\n","    cache_dir=\".\",\n",")\n","\n","\n","saveto = \"./datasets/LJSpeech-1.1\"\n","wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n","\n","id_to_text = {}\n","with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n","    for line in f:\n","        id = line.strip().split(\"|\")[0]\n","        text = line.strip().split(\"|\")[2]\n","        id_to_text[id] = text\n","\n","\n","def get_data(wavs, id_to_text, maxlen=50):\n","    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n","    data = []\n","    for w in wavs:\n","        id = w.split(\"/\")[-1].split(\".\")[0]\n","        if len(id_to_text[id]) < maxlen:\n","            data.append({\"audio\": w, \"text\": id_to_text[id]})\n","    return data\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1dF2-mUGxS5k"},"source":["## Preprocess the dataset"]},{"cell_type":"code","metadata":{"id":"LDAuMT1DxS5k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618310016354,"user_tz":420,"elapsed":275761,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}},"outputId":"284c3589-c01c-4950-bd6a-d4a2db8b303b"},"source":["\n","class VectorizeChar:\n","    def __init__(self, max_len=50):\n","        self.vocab = (\n","            [\"-\", \"#\", \"<\", \">\"]\n","            + [chr(i + 96) for i in range(1, 27)]\n","            + [\" \", \".\", \",\", \"?\"]\n","        )\n","        self.max_len = max_len\n","        self.char_to_idx = {}\n","        for i, ch in enumerate(self.vocab):\n","            self.char_to_idx[ch] = i\n","\n","    def __call__(self, text):\n","        text = text.lower()\n","        text = text[: self.max_len - 2]\n","        text = \"<\" + text + \">\"\n","        pad_len = self.max_len - len(text)\n","        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n","\n","    def get_vocabulary(self):\n","        return self.vocab\n","\n","\n","max_target_len = 200  # all transcripts in out data are < 200 characters\n","data = get_data(wavs, id_to_text, max_target_len)\n","vectorizer = VectorizeChar(max_target_len)\n","print(\"vocab size\", len(vectorizer.get_vocabulary()))\n","\n","\n","def create_text_ds(data):\n","    texts = [_[\"text\"] for _ in data]\n","    text_ds = [vectorizer(t) for t in texts]\n","    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n","    return text_ds\n","\n","\n","def path_to_audio(path):\n","    # spectrogram using stft\n","    audio = tf.io.read_file(path)\n","    audio, _ = tf.audio.decode_wav(audio, 1)\n","    audio = tf.squeeze(audio, axis=-1)\n","    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n","    x = tf.math.pow(tf.abs(stfts), 0.5)\n","    # normalisation\n","    means = tf.math.reduce_mean(x, 1, keepdims=True)\n","    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n","    x = (x - means) / stddevs\n","    audio_len = tf.shape(x)[0]\n","    # padding to 10 seconds\n","    pad_len = 2754\n","    paddings = tf.constant([[0, pad_len], [0, 0]])\n","    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n","    return x\n","\n","\n","def create_audio_ds(data):\n","    flist = [_[\"audio\"] for _ in data]\n","    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n","    audio_ds = audio_ds.map(\n","        path_to_audio, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    )\n","    return audio_ds\n","\n","\n","def create_tf_dataset(data, bs=4):\n","    audio_ds = create_audio_ds(data)\n","    text_ds = create_text_ds(data)\n","    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n","    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n","    ds = ds.batch(bs)\n","    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n","    return ds\n","\n","\n","split = int(len(data) * 0.99)\n","train_data = data[:split]\n","test_data = data[split:]\n","ds = create_tf_dataset(train_data, bs=256)\n","val_ds = create_tf_dataset(test_data, bs=4)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["vocab size 34\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LKN5wzQhxS5l"},"source":["## Callbacks to display predictions"]},{"cell_type":"code","metadata":{"id":"7Y_b9g2MxS5l","executionInfo":{"status":"ok","timestamp":1618310016355,"user_tz":420,"elapsed":275753,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class DisplayOutputs(keras.callbacks.Callback):\n","    def __init__(\n","        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n","    ):\n","        \"\"\"Displays a batch of outputs after every epoch\n","\n","        Args:\n","            batch: A test batch containing the keys \"source\" and \"target\"\n","            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n","            target_start_token_idx: A start token index in the target vocabulary\n","            target_end_token_idx: An end token index in the target vocabulary\n","        \"\"\"\n","        self.batch = batch\n","        self.target_start_token_idx = target_start_token_idx\n","        self.target_end_token_idx = target_end_token_idx\n","        self.idx_to_char = idx_to_token\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if epoch % 5 != 0:\n","            return\n","        source = self.batch[\"source\"]\n","        target = self.batch[\"target\"].numpy()\n","        bs = tf.shape(source)[0]\n","        preds = self.model.generate(source, self.target_start_token_idx)\n","        preds = preds.numpy()\n","        for i in range(bs):\n","            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n","            prediction = \"\"\n","            for idx in preds[i, :]:\n","                prediction += self.idx_to_char[idx]\n","                if idx == self.target_end_token_idx:\n","                    break\n","            print(f\"target:     {target_text.replace('-','')}\")\n","            print(f\"prediction: {prediction}\\n\")\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6YW2xIm6xS5l"},"source":["## Learning rate schedule"]},{"cell_type":"code","metadata":{"id":"mP2lZGyTxS5l","executionInfo":{"status":"ok","timestamp":1618310016357,"user_tz":420,"elapsed":275749,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(\n","        self,\n","        init_lr=0.00001,\n","        lr_after_warmup=0.001,\n","        final_lr=0.00001,\n","        warmup_epochs=15,\n","        decay_epochs=85,\n","        steps_per_epoch=203,\n","    ):\n","        super().__init__()\n","        self.init_lr = init_lr\n","        self.lr_after_warmup = lr_after_warmup\n","        self.final_lr = final_lr\n","        self.warmup_epochs = warmup_epochs\n","        self.decay_epochs = decay_epochs\n","        self.steps_per_epoch = steps_per_epoch\n","\n","    def calculate_lr(self, epoch):\n","        \"\"\" linear warm up - linear decay \"\"\"\n","        warmup_lr = (\n","            self.init_lr\n","            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n","        )\n","        decay_lr = tf.math.maximum(\n","            self.final_lr,\n","            self.lr_after_warmup\n","            - (epoch - self.warmup_epochs)\n","            * (self.lr_after_warmup - self.final_lr)\n","            / (self.decay_epochs),\n","        )\n","        return tf.math.minimum(warmup_lr, decay_lr)\n","\n","    def __call__(self, step):\n","        epoch = step // self.steps_per_epoch\n","        return self.calculate_lr(epoch)\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZOGRnVkxS5m"},"source":["## Create & train the end-to-end model"]},{"cell_type":"code","metadata":{"id":"opKqs6fQxS5m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618320117478,"user_tz":420,"elapsed":10376863,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}},"outputId":"e2d6b7e3-f8b5-48ad-d486-9f3379a4470a"},"source":["batch = next(iter(val_ds))\n","\n","# The vocabulary to convert predicted indices into characters\n","idx_to_char = vectorizer.get_vocabulary()\n","display_cb = DisplayOutputs(\n","    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",")  # set the arguments as per vocabulary index for '<' and '>'\n","\n","model = Transformer(\n","    num_hid=200,\n","    num_head=2,\n","    num_feed_forward=400,\n","    target_maxlen=max_target_len,\n","    num_layers_enc=4,\n","    num_layers_dec=1,\n","    num_classes=34,\n",")\n","loss_fn = tf.keras.losses.CategoricalCrossentropy(\n","    from_logits=True, label_smoothing=0.1,\n",")\n","\n","learning_rate = CustomSchedule(\n","    init_lr=0.00001,\n","    lr_after_warmup=0.001,\n","    final_lr=0.00001,\n","    warmup_epochs=15,\n","    decay_epochs=85,\n","    steps_per_epoch=len(ds),\n",")\n","optimizer = keras.optimizers.Adam(learning_rate)\n","model.compile(optimizer=optimizer, loss=loss_fn)\n","\n","earlystopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","\n","history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb, earlystopping], epochs=100, verbose=2)\n","print(model.evaluate(val_ds))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","51/51 - 165s - loss: 1.9261 - val_loss: 1.8660\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: < tn then s aae n theran s aae tps ti-raea en s s ra s s s e o tgferaeve s be s sln s th tnes is s athi s the thitoutis tm th s n thi- a s tra th rats ers t atgf tnrraeratn tnou s thi>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: < tn then s aae n theran s aae tps ti-raea en s s ra s s s e o tgferaeve s be s sln s th tnes is s athi s the thitoutis tm th s n thi- a s tra th rats ers t atgf tnrraeratn tnou s thi>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: < tn then s aae n theran s aae tps ti-raea en s s ra s s s e o tgferaeve s be s sln s th tnes is s athi s the thitoutis tm th s n thi- a s tra th rats ers t atgf tnrraeratn tnou s thi>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: < tn then s aae n theran s aae tps ti-raea en s s ra s s s e o tgferaeve s be s sln s th tnes is s athi s the thitoutis tm th s n thi- a s tra th rats ers t atgf tnrraeratn tnou s thi>\n","\n","Epoch 2/100\n","51/51 - 157s - loss: 1.5325 - val_loss: 1.5046\n","Epoch 3/100\n","51/51 - 157s - loss: 1.3884 - val_loss: 1.4237\n","Epoch 4/100\n","51/51 - 157s - loss: 1.3395 - val_loss: 1.3914\n","Epoch 5/100\n","51/51 - 157s - loss: 1.3196 - val_loss: 1.3780\n","Epoch 6/100\n","51/51 - 157s - loss: 1.3107 - val_loss: 1.3732\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the as and athe the an the the an the an the the the the an an an the an an the an an the the and an athe the an athe an the the the the the there the anere the thererereren tennedy.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the as and athe the an the the an the are the the the the are the the the the the the the the an the are the the the the are the there there therere.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <the as and the the are the the an the are the the the the are the the the the the the the the are are an the therere.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the as and athe the an the the an the are the the the the an the the the an the are the are the an the the the the are the thererererere.>\n","\n","Epoch 7/100\n","51/51 - 157s - loss: 1.3055 - val_loss: 1.3672\n","Epoch 8/100\n","51/51 - 157s - loss: 1.3026 - val_loss: 1.3642\n","Epoch 9/100\n","51/51 - 157s - loss: 1.2992 - val_loss: 1.3620\n","Epoch 10/100\n","51/51 - 157s - loss: 1.2942 - val_loss: 1.3556\n","Epoch 11/100\n","51/51 - 157s - loss: 1.2847 - val_loss: 1.3408\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the sore the the the the s ase ande the the the as the the the the as the the ase the the the as the the the ase the ase as the the the the the the se as.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the there the the thererofofof arererererof pre.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <who was the the the s the the the the the the the the the the soure the the the the the o the as the orore the are the orore theroroure.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the as the the the as as asofof the the the the the the the the the the the the the the as the the as the the as the as the the asofofof the ofofourou.>\n","\n","Epoch 12/100\n","51/51 - 157s - loss: 1.2689 - val_loss: 1.3217\n","Epoch 13/100\n","51/51 - 157s - loss: 1.2482 - val_loss: 1.2953\n","Epoch 14/100\n","51/51 - 157s - loss: 1.2226 - val_loss: 1.2691\n","Epoch 15/100\n","51/51 - 157s - loss: 1.1783 - val_loss: 1.2112\n","Epoch 16/100\n","51/51 - 157s - loss: 1.0956 - val_loss: 1.1212\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the in the and the the and the the and ther and to the man the the man the susssint o there and to the the man s tre mand t tre the t tre t t t t t se there the t terererent kennedyecerr.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the defer compatin to he compatin to the he compatin to the compatin to the the cof he histr he hat t the the the tour st the sthe the sthe the t the sthe our.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <the cis of the the re ref the rent the rear wals wals a the re ref the re the re rento the re the ref the the re re the tof the the.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the was no the the selfice omplin to the self the the so the self the cat the so the so so the the so prif the s the so o o the so the se oun so se so so so.>\n","\n","Epoch 17/100\n","51/51 - 157s - loss: 0.9976 - val_loss: 1.0017\n","Epoch 18/100\n","51/51 - 157s - loss: 0.8985 - val_loss: 0.9176\n","Epoch 19/100\n","51/51 - 157s - loss: 0.8257 - val_loss: 0.8590\n","Epoch 20/100\n","51/51 - 157s - loss: 0.7769 - val_loss: 0.8290\n","Epoch 21/100\n","51/51 - 157s - loss: 0.7429 - val_loss: 0.8051\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the intervanges iver duse intervanges were success were hand intervanges where suped intervanned intervanent intery compris wison s.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor streer started the because leffe chim in the because leffe chim in the back of thin the because lefter him in in thisthisthack>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the brises of the brid, and the briso the brid, and the brises of the brid a reven the brides of the been to wallshed told te.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they was not selft the prent on ot so capplet so capplet so capplet so capplet so capplet so capplet so capplet so capplet the.>\n","\n","Epoch 22/100\n","51/51 - 157s - loss: 0.7168 - val_loss: 0.7869\n","Epoch 23/100\n","51/51 - 157s - loss: 0.6954 - val_loss: 0.7671\n","Epoch 24/100\n","51/51 - 157s - loss: 0.6713 - val_loss: 0.7527\n","Epoch 25/100\n","51/51 - 158s - loss: 0.6534 - val_loss: 0.7467\n","Epoch 26/100\n","51/51 - 157s - loss: 0.6403 - val_loss: 0.7443\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the intervery comprisons were severable intervanage comprisons were succede manatery comprisons were succede intervanage interve med merithe thente is.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governorstred the the governed the the governor streged the because he felef stright him in the because him in the because hin thin thin thin thin thin thit>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the rever rever walls have been the briges of the breagen the breagen the bright the rever wallsh have dison the beer wallleate.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they was notrace of the prent by lifting had been so complet so complet so complet so complete so complete so complete so compleeeeaneath.>\n","\n","Epoch 27/100\n","51/51 - 157s - loss: 0.6293 - val_loss: 0.7327\n","Epoch 28/100\n","51/51 - 157s - loss: 0.6195 - val_loss: 0.7264\n","Epoch 29/100\n","51/51 - 157s - loss: 0.6079 - val_loss: 0.7173\n","Epoch 30/100\n","51/51 - 157s - loss: 0.5983 - val_loss: 0.7173\n","Epoch 31/100\n","51/51 - 157s - loss: 0.5902 - val_loss: 0.7140\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <at demisive lie inter oduced in the inter oduced in the inter oduced in the inter valuused inter val of prisons were ture vanage in min min in intof mintontons.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor strigh completed to look back him in the back him in the back him in the back him in the back him in the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but treses of the mat tols have bridises opeared an the bridises opearn an the bridises opearn an the been towashed tol have beeeing.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they so complet so comple if inself ifle it so complet so case of the prent was notrace of the prent on a trace on of the prent onono one o o.>\n","\n","Epoch 32/100\n","51/51 - 157s - loss: 0.5813 - val_loss: 0.7150\n","Epoch 33/100\n","51/51 - 157s - loss: 0.5732 - val_loss: 0.7071\n","Epoch 34/100\n","51/51 - 157s - loss: 0.5658 - val_loss: 0.7130\n","Epoch 35/100\n","51/51 - 157s - loss: 0.5600 - val_loss: 0.7040\n","Epoch 36/100\n","51/51 - 157s - loss: 0.5526 - val_loss: 0.7015\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the interval interval anages of prisons were succed interval atury changes of prisons were succed interval of prisons were succed st interins.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governorst are nevern, because him in the back him in the back him in the back him in the back him in the back him in the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <otres have been the bridisappyared the ridgges have been the bridises of the bridgges have been to allshed ridisappy.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so complet so complet so complet selfiting had been so complet so complet so complet so complet so complet so tene ssssssssssssssa.>\n","\n","Epoch 37/100\n","51/51 - 157s - loss: 0.5467 - val_loss: 0.6995\n","Epoch 38/100\n","51/51 - 157s - loss: 0.5401 - val_loss: 0.6972\n","Epoch 39/100\n","51/51 - 157s - loss: 0.5350 - val_loss: 0.6967\n","Epoch 40/100\n","51/51 - 157s - loss: 0.5309 - val_loss: 0.6975\n","Epoch 41/100\n","51/51 - 157s - loss: 0.5264 - val_loss: 0.6980\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the intervancessive leator turry changess of leat agent of prisons were successive lie inter duccessive leat agent of prisons were ont iveryive t mint ins.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor strigne the tenevern, because he felsome to left helsome to left he telsompleted the terned the tellter cime striter te.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the rever walls have been tol way a bridgges have been to wallshed a been tol wash ave been to wallshed a a been to washe.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they was notrace of the prent was noth rifle it in the so case of the prent when a the was noth rifle it in the so case of the pren the th.>\n","\n","Epoch 42/100\n","51/51 - 157s - loss: 0.5218 - val_loss: 0.7003\n","Epoch 43/100\n","51/51 - 157s - loss: 0.5182 - val_loss: 0.6935\n","Epoch 44/100\n","51/51 - 157s - loss: 0.5163 - val_loss: 0.6920\n","Epoch 45/100\n","51/51 - 157s - loss: 0.5124 - val_loss: 0.6968\n","Epoch 46/100\n","51/51 - 157s - loss: 0.5081 - val_loss: 0.6983\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the intervery combry cevannats be a intervanator a changes of prisons were severy combly into the must in to the muste inter ese intontonton mintonte mis.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governed to look back him in the back him in the back him in the back him in the back him in the because him in the back him in back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <the river wayges have been told the riverwalls have been to wayges have been to way.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they was noton the rifle it self when adby lifting had been so complet so complet self ton the rifle it in this case of the print ule the ton the the the thedenerine onenengradeneratherint d d d d s \n","\n","Epoch 47/100\n","51/51 - 157s - loss: 0.5055 - val_loss: 0.6943\n","Epoch 48/100\n","51/51 - 157s - loss: 0.5009 - val_loss: 0.6960\n","Epoch 49/100\n","51/51 - 157s - loss: 0.4951 - val_loss: 0.6960\n","Epoch 50/100\n","51/51 - 157s - loss: 0.4910 - val_loss: 0.6953\n","Epoch 51/100\n","51/51 - 157s - loss: 0.4863 - val_loss: 0.6927\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the interver it man salused it mancive lieter very comprehant of prisons were successive lievel intervil interval inter very comprisons.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor strigh him in the back him in the back him in the back him in the back him in the back him in the back him in the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the ridises of the been the bridises of the been to wall shave been to wallsh have been to wall shave been to a peare.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they was notraced that they was iden so the prent when examine so completing a had been so completon the rifle was iden so completin by by on the.>\n","\n","Epoch 52/100\n","51/51 - 157s - loss: 0.4838 - val_loss: 0.6889\n","Epoch 53/100\n","51/51 - 157s - loss: 0.4808 - val_loss: 0.6916\n","Epoch 54/100\n","51/51 - 157s - loss: 0.4778 - val_loss: 0.6878\n","Epoch 55/100\n","51/51 - 159s - loss: 0.4736 - val_loss: 0.6880\n","Epoch 56/100\n","51/51 - 157s - loss: 0.4684 - val_loss: 0.6898\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <that it musp lieator of prisons were success of leater of prisons were success of leater exesse of leater exessal be a dmitted must int s mant atomint s mives.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started the the to look him in the to look him in the because he felsome thing streger started the to look him in the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but trosh have been the been the brigest have been to walls have been to walls have been to walls have disappeared,>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they was exammon a ton the rifle it in the rifle it in the rifle it in the rifle it in the riflet un othe prent by lit in the riflet un the the the on the o.>\n","\n","Epoch 57/100\n","51/51 - 158s - loss: 0.4641 - val_loss: 0.6935\n","Epoch 58/100\n","51/51 - 157s - loss: 0.4602 - val_loss: 0.6967\n","Epoch 59/100\n","51/51 - 157s - loss: 0.4593 - val_loss: 0.6977\n","Epoch 60/100\n","51/51 - 157s - loss: 0.4587 - val_loss: 0.7034\n","Epoch 61/100\n","51/51 - 157s - loss: 0.4569 - val_loss: 0.7016\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the intervanages of prisons were success of prisons were succcess of prisons were success of prisons were success of prisons were succconsons.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governerst overner strik him in the to look back him in the back him in the back him in the back him in the back him in the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the river wall sthe the been tol, and the been to wayges have dises of the been told#s have dises of the been to a lll shaved dise.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lit that the lit so complet so complet so complet so complet so complet when a rifle it in this case of the print by lit in the rifl the uthe,>\n","\n","Epoch 62/100\n","51/51 - 157s - loss: 0.4534 - val_loss: 0.6981\n","Epoch 63/100\n","51/51 - 157s - loss: 0.4495 - val_loss: 0.7003\n","Epoch 64/100\n","51/51 - 157s - loss: 0.4485 - val_loss: 0.6991\n","33/33 [==============================] - 2s 52ms/step - loss: 0.6878\n","0.6878065466880798\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lQLX44PxxS5m"},"source":["In practice, you should train for around 100 epochs or more.\n","\n","Some of the predicted text at or around epoch 35 may look as follows:\n","```\n","target:     <as they sat in the car, frazier asked oswald where his lunch was>\n","prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n","\n","target:     <under the entry for may one, nineteen sixty,>\n","prediction: <under the introus for may monee, nin the sixty,>\n","```"]},{"cell_type":"markdown","metadata":{"id":"3zXmsmrbyEH-"},"source":["# Pruno1D"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gGzyzVbyInf","executionInfo":{"status":"ok","timestamp":1618320121140,"user_tz":420,"elapsed":10380517,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}},"outputId":"df52c17d-06f8-4d40-b957-641247908d38"},"source":["!pip install -q git+https://github.com/LanceNorskog/keras-pruno.git\n","from keras_pruno import Pruno1D"],"execution_count":11,"outputs":[{"output_type":"stream","text":["  Building wheel for keras-pruno (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rBENfj5vyUNi","executionInfo":{"status":"ok","timestamp":1618320121141,"user_tz":420,"elapsed":10380510,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class SpeechFeatureEmbeddingPruno(layers.Layer):\n","    def __init__(self, num_hid=64, maxlen=100):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv2 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv3 = tf.keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.pruno1d = Pruno1D(similarity=0.65)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.pruno1d(x)\n","        return self.conv3(x)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGb9SXSDyx9A","executionInfo":{"status":"ok","timestamp":1618320121141,"user_tz":420,"elapsed":10380504,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":["\n","class TransformerPruno(keras.Model):\n","    def __init__(\n","        self,\n","        num_hid=64,\n","        num_head=2,\n","        num_feed_forward=128,\n","        source_maxlen=100,\n","        target_maxlen=100,\n","        num_layers_enc=4,\n","        num_layers_dec=1,\n","        num_classes=10,\n","    ):\n","        super().__init__()\n","        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n","        self.num_layers_enc = num_layers_enc\n","        self.num_layers_dec = num_layers_dec\n","        self.target_maxlen = target_maxlen\n","        self.num_classes = num_classes\n","\n","        self.enc_input = SpeechFeatureEmbeddingPruno(num_hid=num_hid, maxlen=source_maxlen)\n","        self.dec_input = TokenEmbedding(\n","            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n","        )\n","\n","        self.encoder = keras.Sequential(\n","            [self.enc_input]\n","            + [\n","                TransformerEncoder(num_hid, num_head, num_feed_forward)\n","                for _ in range(num_layers_enc)\n","            ]\n","        )\n","\n","        for i in range(num_layers_dec):\n","            setattr(\n","                self,\n","                f\"dec_layer_{i}\",\n","                TransformerDecoder(num_hid, num_head, num_feed_forward),\n","            )\n","\n","        self.classifier = layers.Dense(num_classes)\n","\n","    def decode(self, enc_out, target):\n","        y = self.dec_input(target)\n","        for i in range(self.num_layers_dec):\n","            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n","        return y\n","\n","    def call(self, inputs):\n","        source = inputs[0]\n","        target = inputs[1]\n","        x = self.encoder(source)\n","        y = self.decode(x, target)\n","        return self.classifier(y)\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_metric]\n","\n","    def train_step(self, batch):\n","        \"\"\"Processes one batch inside model.fit().\"\"\"\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        with tf.GradientTape() as tape:\n","            preds = self([source, dec_input])\n","            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def test_step(self, batch):\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        preds = self([source, dec_input])\n","        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def generate(self, source, target_start_token_idx):\n","        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n","        bs = tf.shape(source)[0]\n","        enc = self.encoder(source)\n","        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n","        dec_logits = []\n","        for i in range(self.target_maxlen - 1):\n","            dec_out = self.decode(enc, dec_input)\n","            logits = self.classifier(dec_out)\n","            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n","            dec_logits.append(last_logit)\n","            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n","        return dec_input\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDnkPZxLzoPC","executionInfo":{"status":"ok","timestamp":1618330219561,"user_tz":420,"elapsed":20478918,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}},"outputId":"fa29eec7-1679-4c04-ae6e-c2239c461a94"},"source":["batch = next(iter(val_ds))\n","\n","# The vocabulary to convert predicted indices into characters\n","idx_to_char = vectorizer.get_vocabulary()\n","display_cb = DisplayOutputs(\n","    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",")  # set the arguments as per vocabulary index for '<' and '>'\n","\n","model = TransformerPruno(\n","    num_hid=200,\n","    num_head=2,\n","    num_feed_forward=400,\n","    target_maxlen=max_target_len,\n","    num_layers_enc=4,\n","    num_layers_dec=1,\n","    num_classes=34,\n",")\n","loss_fn = tf.keras.losses.CategoricalCrossentropy(\n","    from_logits=True, label_smoothing=0.1,\n",")\n","\n","learning_rate = CustomSchedule(\n","    init_lr=0.00001,\n","    lr_after_warmup=0.001,\n","    final_lr=0.00001,\n","    warmup_epochs=15,\n","    decay_epochs=85,\n","    steps_per_epoch=len(ds),\n",")\n","optimizer = keras.optimizers.Adam(learning_rate)\n","model.compile(optimizer=optimizer, loss=loss_fn)\n","\n","earlystopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","\n","history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb, earlystopping], epochs=100, verbose=2)\n","print(model.evaluate(val_ds))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["__init__: similarity: 0.65\n","Epoch 1/100\n","Pruno1D.build: input_shape: (None, None, 200)\n","Pruno1D.build: self.fmap_shape: (None,)\n","51/51 - 163s - loss: 1.8834 - val_loss: 1.8605\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <ea t ahe hen oen ea scen m e hno tde e hs utdaolt ae ititloy td tv he e futt thde e ne en o e tlt e the tr ue oe xt ut e ths e ue e npee  eaa hs e tse oae s e he e og en e e o it e  hn he te olad o g\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <ea t e trrtr owr ea scen m e hno tde e hs utdaolt ae ititloy td tm he e futco hde e ne en o e tlt t the tr ue oe xt ut e ths e ue e npee  eaa hs e tse oae s e he e og en e e o it e  hn he tj iiad o g\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <ea t ahe hen owr ea scen m e hno tde e hs utdaolt ae ititloy td tm he e futco hde e ne en o e tlt t the tr ue oe xt ut e ths e ue e npee  eaa hs e tse oae s e he e og e e f-e itl e  hn he tj olad o g\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <ea t ahe hen oen ea scen m e hno tde e hs utdaolt ae ititloy td tm he e futco hde e ne en o e tlt t the tr ue oe xt ut e ths e ue e npee  eaa hs e tse oae s e he e og e e f-e itl e  hn he tj olad o g\n","\n","Epoch 2/100\n","51/51 - 157s - loss: 1.5298 - val_loss: 1.5055\n","Epoch 3/100\n","51/51 - 157s - loss: 1.3873 - val_loss: 1.4203\n","Epoch 4/100\n","51/51 - 157s - loss: 1.3365 - val_loss: 1.3886\n","Epoch 5/100\n","51/51 - 157s - loss: 1.3177 - val_loss: 1.3756\n","Epoch 6/100\n","51/51 - 157s - loss: 1.3093 - val_loss: 1.3704\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the as the as and the the the as the an as the as the the an as the an as the the the the the an an the the the the the as the the as the as there ase aserererererederereres ofinedy.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the as the as and the the the an the the the the an an the the the an as the an the the an the are an the an the the the the the and the the there as.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <the as the athe the the the the the the as the are the the the the an an the the the the the the the the the the the the therere ane.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the as the as and the the the as the an as the and an the an the as the an the an the the the an the the the the the the the the as thererererere teres.>\n","\n","Epoch 7/100\n","51/51 - 157s - loss: 1.3052 - val_loss: 1.3652\n","Epoch 8/100\n","51/51 - 157s - loss: 1.3014 - val_loss: 1.3618\n","Epoch 9/100\n","51/51 - 157s - loss: 1.2981 - val_loss: 1.3595\n","Epoch 10/100\n","51/51 - 157s - loss: 1.2924 - val_loss: 1.3523\n","Epoch 11/100\n","51/51 - 157s - loss: 1.2797 - val_loss: 1.3343\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the se the the the se the the the the sore the the the the the the se are the the the the the se the sofofof the the the the the the the the the the the se there fopresident kennedy.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the the the the the the the the the the the the the the the the the the the the the the the the the the the the there s sore the s the prere therofore soforerorere prererert kennedy.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <the the the the the the sore the the the the sore the the the the the the the the the the the sofore the the pre the the the the ore the the the ofoforere.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the se the the the se the the sofore the the the the the the the the the se se the sofofo the se the s the the se the sore the the the sofofore the the sore.>\n","\n","Epoch 12/100\n","51/51 - 157s - loss: 1.2656 - val_loss: 1.3160\n","Epoch 13/100\n","51/51 - 157s - loss: 1.2425 - val_loss: 1.2964\n","Epoch 14/100\n","51/51 - 158s - loss: 1.2096 - val_loss: 1.2466\n","Epoch 15/100\n","51/51 - 157s - loss: 1.1509 - val_loss: 1.1695\n","Epoch 16/100\n","51/51 - 157s - loss: 1.0548 - val_loss: 1.0640\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <the nentred the manter the manter the manter the manter the manter the mand the mand the manter the mand the mand ter to te te te te te te te te te te me te te te teres.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the dof the trical the the the dor the the compler the the cof the the courd the the the dor the plerer the complerer the.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but tris the tre the the butrices of the tres of the trear the bre the bre the brere walls the brere walls the walllls s te terererere.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the was the prifted the was the priftele the sof the priften the trating the sof the trating the sof the the prin to the t the.>\n","\n","Epoch 17/100\n","51/51 - 158s - loss: 0.9589 - val_loss: 0.9784\n","Epoch 18/100\n","51/51 - 157s - loss: 0.8718 - val_loss: 0.8940\n","Epoch 19/100\n","51/51 - 157s - loss: 0.8115 - val_loss: 0.8554\n","Epoch 20/100\n","51/51 - 157s - loss: 0.7646 - val_loss: 0.8200\n","Epoch 21/100\n","51/51 - 157s - loss: 0.7321 - val_loss: 0.7972\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <and intervery comprisons were succe of peary comprisons were succe of pean at man a the man at inter dure duse into the man the the te tore the the the inte theress.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governer stried the becaus holder stright the because heft the in the feleted the because he feleted the backe a complle the toton ton the the.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the brison the briver walls have been to the briver walls have been to the brive been the brive been to the brined the.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <they was not self a the lifty was not self at so not tome trace of the prind by lifte the princelf the prind by lits that s the the the the the a.>\n","\n","Epoch 22/100\n","51/51 - 157s - loss: 0.7068 - val_loss: 0.7802\n","Epoch 23/100\n","51/51 - 157s - loss: 0.6842 - val_loss: 0.7661\n","Epoch 24/100\n","51/51 - 157s - loss: 0.6653 - val_loss: 0.7536\n","Epoch 25/100\n","51/51 - 157s - loss: 0.6520 - val_loss: 0.7470\n","Epoch 26/100\n","51/51 - 157s - loss: 0.6374 - val_loss: 0.7385\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <but intervery coment intervery coment intervery coment of prisonsive an a comprisons recessive a very coment intervery comenthes,>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governer stried the left the back of the him in the becauause him in the back overned to striated to strien the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <butris of the briver was have been torn are was have been to the briver was have been tor was have been tor was have been thalealealeal.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so complets that the lifting had been so complets that the lifting had been so complets that theras of se.>\n","\n","Epoch 27/100\n","51/51 - 157s - loss: 0.6256 - val_loss: 0.7341\n","Epoch 28/100\n","51/51 - 157s - loss: 0.6172 - val_loss: 0.7275\n","Epoch 29/100\n","51/51 - 157s - loss: 0.6072 - val_loss: 0.7212\n","Epoch 30/100\n","51/51 - 158s - loss: 0.5972 - val_loss: 0.7132\n","Epoch 31/100\n","51/51 - 157s - loss: 0.5887 - val_loss: 0.7170\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <a neacted manacessive and i the coment of prisons were succed intervery coment of prisons were succed intervery coment of printhe thery the thenthe anthinthes.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started the back o in the back overnor started the back a in the back overnersted the back of the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <butres have been the been towalsh ave been towalshed have been towals have been towalsh of dis have been towalsh way.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so compleate so complets that therace of the prine was ime was ime was implets that therace of the prind the.>\n","\n","Epoch 32/100\n","51/51 - 157s - loss: 0.5814 - val_loss: 0.7186\n","Epoch 33/100\n","51/51 - 157s - loss: 0.5749 - val_loss: 0.7172\n","Epoch 34/100\n","51/51 - 157s - loss: 0.5688 - val_loss: 0.7070\n","Epoch 35/100\n","51/51 - 157s - loss: 0.5613 - val_loss: 0.7039\n","Epoch 36/100\n","51/51 - 157s - loss: 0.5564 - val_loss: 0.6968\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <a then intervery comprisons were succed interval prisons were succed intervery han a intervery hanace of prisons were succed we pre pre pre onsonsons.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started the back of started the back of started the back of started the the the the back of started the the the he him.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the river sup have been to the rivered the brivered the briver way.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so complets that they so complet so complets that the lifting his of the prent was idelf at the prent wasonononononononononono.>\n","\n","Epoch 37/100\n","51/51 - 157s - loss: 0.5460 - val_loss: 0.7041\n","Epoch 38/100\n","51/51 - 158s - loss: 0.5401 - val_loss: 0.7030\n","Epoch 39/100\n","51/51 - 157s - loss: 0.5356 - val_loss: 0.6933\n","Epoch 40/100\n","51/51 - 158s - loss: 0.5307 - val_loss: 0.6912\n","Epoch 41/100\n","51/51 - 157s - loss: 0.5246 - val_loss: 0.6930\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <a thing to the manages were succoment of prisons were succoment of prisons were succoment of prisons were succoment of prisons of w wheses ons.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started the back of strigh him in the back o strigh him in the back o strigh him devernorsted the back o strigh himan him.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but the but the briverod, and the briver walls have been to river walls have been towalsh of disuppeared, and the brivered,>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so complets that the lifting lifting lifting lifting this of the prifle it was iden so complets that the s a.>\n","\n","Epoch 42/100\n","51/51 - 158s - loss: 0.5191 - val_loss: 0.6913\n","Epoch 43/100\n","51/51 - 157s - loss: 0.5131 - val_loss: 0.6877\n","Epoch 44/100\n","51/51 - 157s - loss: 0.5078 - val_loss: 0.6837\n","Epoch 45/100\n","51/51 - 157s - loss: 0.5032 - val_loss: 0.6825\n","Epoch 46/100\n","51/51 - 158s - loss: 0.4990 - val_loss: 0.6833\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <a thing to the mana a ssassivalunus dinturable very compredues were suchment of prison dint inturable inturable inturry compres ante te ans.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started to look baten back overnor started to look back o bat him and back o back overnor started to look.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <buttres have been to river way ges have been torn or way.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so completh was notrace of trace of the print was notrace of the print was notrace of ton the rifle itsel.>\n","\n","Epoch 47/100\n","51/51 - 157s - loss: 0.4934 - val_loss: 0.6817\n","Epoch 48/100\n","51/51 - 157s - loss: 0.4896 - val_loss: 0.6775\n","Epoch 49/100\n","51/51 - 157s - loss: 0.4851 - val_loss: 0.6754\n","Epoch 50/100\n","51/51 - 157s - loss: 0.4806 - val_loss: 0.6736\n","Epoch 51/100\n","51/51 - 156s - loss: 0.4753 - val_loss: 0.6717\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <a thing the intervely intervely intervely intervel, salurce succed into the mananages were succed intervely intervely intervely t t t t ans t te he ans.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started the turnor started the turnor started the turned but hind the back overnor stage hin the back him in the be him him him himp.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <but there walls have been towals have been towals have been towals have been towalls have been towals have been towals have beeey.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so complete in so completself itself itself itself itself itself itself itself it was examin so completself calf the.>\n","\n","Epoch 52/100\n","51/51 - 157s - loss: 0.4736 - val_loss: 0.6698\n","Epoch 53/100\n","51/51 - 156s - loss: 0.4717 - val_loss: 0.6702\n","Epoch 54/100\n","51/51 - 156s - loss: 0.4692 - val_loss: 0.6690\n","Epoch 55/100\n","51/51 - 156s - loss: 0.4660 - val_loss: 0.6700\n","Epoch 56/100\n","51/51 - 156s - loss: 0.4628 - val_loss: 0.6748\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <a bensivanges were success of lere success of lere to the manages were changes were changes were changes were changes were to thans w we weres.>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started to look pleted to look pleted to look pleted to look pleted to look pleted to look bathing strighalder, buck.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <butress of the briver walls hof disup the river walls have been tor walls have been tor walls have been tor walls have been toroy.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so complete self when iscase of trace of trace of trace of trace of trace of the print on on oplets that the onon ononon ononononon h.>\n","\n","Epoch 57/100\n","51/51 - 156s - loss: 0.4600 - val_loss: 0.6701\n","Epoch 58/100\n","51/51 - 156s - loss: 0.4571 - val_loss: 0.6731\n","Epoch 59/100\n","51/51 - 156s - loss: 0.4534 - val_loss: 0.6720\n","Epoch 60/100\n","51/51 - 157s - loss: 0.4498 - val_loss: 0.6728\n","Epoch 61/100\n","51/51 - 156s - loss: 0.4476 - val_loss: 0.6806\n","target:     <but in the interval very comprehensive and, i think it must be admitted, salutary changes were successively introduced into the management of prisons.>\n","prediction: <a neand intervery coment of prisons, in to the manages wed into the manages wed into the manages were changes were chance were tha of of weas>\n","\n","target:     <the governor started to look back over his left shoulder, but he never completed the turn because he felt something strike him in the back.>\n","prediction: <the governor started to left someted to look becaus he felt someted to look becaus he felt something the back.>\n","\n","target:     <but the river walls have disappeared, and the buttresses of the bridges have been torn or washed away.>\n","prediction: <butres have been tor river washave been tornornor washave been tor washave been tornornornornorn or washave been tornornornornorne.>\n","\n","target:     <the lifting had been so complete in this case that there was no trace of the print on the rifle itself when it was examined by latona.>\n","prediction: <the lifting had been so complete in this cased that they was examine so complete in this cased that they was examined by litself ce.>\n","\n","Epoch 62/100\n","51/51 - 156s - loss: 0.4454 - val_loss: 0.6780\n","Epoch 63/100\n","51/51 - 157s - loss: 0.4427 - val_loss: 0.6706\n","Epoch 64/100\n","51/51 - 157s - loss: 0.4404 - val_loss: 0.6715\n","33/33 [==============================] - 2s 50ms/step - loss: 0.6690\n","0.6690106391906738\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C7MiiJfGK0mQ","executionInfo":{"status":"ok","timestamp":1618330219563,"user_tz":420,"elapsed":20478917,"user":{"displayName":"Lance N.","photoUrl":"","userId":"10641691903871358793"}}},"source":[""],"execution_count":14,"outputs":[]}]}